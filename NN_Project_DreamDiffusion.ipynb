{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khwYNvZdn5lv"
      },
      "source": [
        "# Neural Networks Project\n",
        "Neural Networks course at Sapienza University of Rome\n",
        "## DreamDiffusion: Generating High-Quality Images from Brain EEG Signals\n",
        "\n",
        "*   Paolo Caruso 1843152 \n",
        "*   Cristian Fioravanti 1861593\n",
        "\n",
        "The objective of this notebook is to reproduce and reimplement the work carried out by the authors of the \"<b>DreamDiffusion</b>\" (https://arxiv.org/pdf/2306.16934.pdf) paper, exploring the key steps of the proposed algorithm and providing a practical and accessible version for the generation of images starting from EEG signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper contribution\n",
        "The authors of the papers presents a method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text.\n",
        "DreamDiffusion leverages pretrained text-to-image Stable Diffusion model and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs.\n",
        "The authors' intuition is to define a Masked Autoencoder which randomly mask a proportion of tokens and then reconstruct those masked ones in the time domain. In this way, the pre-trained encoder learns a deep understanding of EEG data across different people and various brain activities. Stable Diffusion itself uses CLIP’s text encoder to generate text embeddings, which are quite different from the masked pre-trained EEG embeddings in the previous stage. We leverage CLIP’s image encoder to extract rich image embeddings that align well with CLIP text embeddings. Those CLIP image embeddings are then used to further optimize EEG embedding representations. Therefore, the refined EEG feature embeddings can be well aligned with the CLIP image and text embeddings, and are more suitable for SD image generation, which in turn improves the quality of generated images.\n",
        "\n",
        "The structure of the proposed method is presented here: ![Schema](docs/schema.png)\n",
        "\n",
        "So the method is composed by three different sections:\n",
        "1. Masked signal pre-training for effective and robust EEG representations: in which  masked signal are leveraged modeling with lots of noisy EEG data to train an EEG encoder to extract contextual knowledge.\n",
        "2. Fine-tuning with limited EEG-image pairs with pre-trained Stable Diffusion: in which the resulting EEG encoder is then employed to provide conditional features for Stable Diffusion via the cross-attention mechanism.\n",
        "3. Aligning the EEG, text, and image spaces using CLIP encoders: in which the EEG, text, and image embedding spaces are aligned by reducing the distance between EEG embeddings and CLIP image embeddings during fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our implematation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets\n",
        "* A large amounts of [EEG data](https://studentiunict-my.sharepoint.com/personal/concetto_spampinato_unict_it/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fconcetto%5Fspampinato%5Funict%5Fit%2FDocuments%2Fsito%5FPeRCeiVe%2Fdatasets%2Feeg%5Fcvpr%5F2017&ga=1) used to pre-training in order to achieve an efficient Masked AutoEncoder.\n",
        "* A set of [EEG-image pairs](https://studentiunict-my.sharepoint.com/personal/concetto_spampinato_unict_it/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fconcetto%5Fspampinato%5Funict%5Fit%2FDocuments%2Fsito%5FPeRCeiVe%2Fdatasets%2Feeg%5Fcvpr%5F2017&ga=1) used to accurately align the EEG features with existing text embedding. These EEG are obtained from some subjects while they were shown 2000 images belonging to 40 different categories of objects from the ImageNet\n",
        "dataset.\n",
        "* A subset of [ImageNet](https://drive.google.com/file/d/1y7I9bG1zKYqBM94odcox_eQjnP9HGo9-/view?usp=drive_link) containing the images showed to subsets.\n",
        "\n",
        "All these datasets are downloaded, prepared and splitted using specific dataloader (CODICE DI code/dataset.py e 1-pretrain.py?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Masked signal pre-training for effective and robust EEG representations\n",
        "In order to implement this section we define a MAE (Masked AutoEncoder) which is composed by:\n",
        "* Encoder: to transform the input EEG data in a latent representation. the input is divided in unidimensional patches using a PatchEmbed1D. This is composed by a 1D convolution layer. The patches are elaborated by a series Transformers Blocks: each of which applies multi-head self-attention followed by a feedforward neural network. During encoding, a random mask is applied in order to mask a portion of the input information. This process helps the model learn representations that are robust and invariant to random variations in the input data.\n",
        "* Decoder: The decoder takes the latent representation generated by the encoder and tries to reconstruct the original input. It uses a series of encoder-like Transformer blocks to decode the latent representation. During decoding, mask tokens are added to handle the reconstruction of patches that were masked during encoding.\n",
        "* Loss Function: The loss function is a form of Mean Squared Error (MSE) calculated between the original EEG signal and the reconstructed EEG signal. However, the loss is only calculated for patches that were masked during encoding, to encourage the model to focus on those patches during reconstruction.\n",
        "\n",
        "Here the code of the definition of the MAE. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import utils as ut\n",
        "from timm.models.vision_transformer import Block\n",
        "\n",
        "class PatchEmbed1D(nn.Module):\n",
        "    def __init__(self, time_len=224, patch_size=1, in_chans=128, embed_dim=256):\n",
        "        super().__init__()\n",
        "        num_patches = time_len // patch_size  # 224\n",
        "        self.patch_shape = patch_size\n",
        "        self.time_len = time_len\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv1d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "\n",
        "        x = (\n",
        "            # patch projected in the dimension of the embedding's dimention (embed_dim)\n",
        "            self.proj(x).transpose(1, 2).contiguous() #from torch.Size([1, 1024, 128]) torch.Size([1, 128, 1024])\n",
        "        )  # put embed_dim at the last dimension\n",
        "\n",
        "        return x\n",
        "    \n",
        "class MaskedAutoEncoderEEG(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time_len=512,\n",
        "        patch_size=4,\n",
        "        embed_dim=1024,\n",
        "        in_chans=128,\n",
        "        depth=24,\n",
        "        num_heads=16,\n",
        "        decoder_embed_dim=512,\n",
        "        decoder_depth=8,\n",
        "        decoder_num_heads=16,\n",
        "        mlp_ratio=1.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        focus_range=None,\n",
        "        focus_rate=None,\n",
        "        img_recon_weight=1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed1D(\n",
        "            time_len, patch_size, in_chans, embed_dim)\n",
        "\n",
        "        num_patches = int(time_len / patch_size)\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # torch.Size([1, 1, 1024])\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False #torch.Size([1, 129, 1024])\n",
        "        )  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                 Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "    \n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "    \n",
        "        self.decoder_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False\n",
        "        )  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [\n",
        "               Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
        "               for i in range(decoder_depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(\n",
        "            decoder_embed_dim, in_chans * patch_size, bias=True\n",
        "        )  # encoder to decoder\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.focus_range = focus_range\n",
        "        self.focus_rate = focus_rate\n",
        "        self.img_recon_weight = img_recon_weight\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        pos_embed = ut.get_1d_sincos_pos_embed(\n",
        "            self.pos_embed.shape[-1], self.num_patches, cls_token=True\n",
        "        )\n",
        "        decoder_pos_embed = ut.get_1d_sincos_pos_embed(\n",
        "            self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True\n",
        "        )\n",
        "        self.pos_embed.data.copy_(\n",
        "            torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
        "        )  # trasforma gli embedding in tensor\n",
        "\n",
        "        self.decoder_pos_embed.data.copy_(\n",
        "            torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n",
        "        )  # trasforma gli embedding in tensor\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(\n",
        "            self._init_weights\n",
        "        )  # _init_weights viene applicato per inizializzare i pesi e bias per i livelli lineari e la normalizzazione del livello all'interno del modello\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Prende un tensore di immagini e lo converte in un formato di patch\n",
        "    def patchify(self, imgs):\n",
        "        \"\"\"\n",
        "        imgs: (N, 1, num_voxels)\n",
        "        imgs: [N, chan, T]\n",
        "        x: (N, L, patch_size)\n",
        "        x: [N, chan * 4, T/4]\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size\n",
        "        assert imgs.ndim == 3 and imgs.shape[1] % p == 0\n",
        "\n",
        "        # h = imgs.shape[2] // p\n",
        "        x = imgs.reshape(shape=(imgs.shape[0], imgs.shape[1] // p, -1))\n",
        "        return x\n",
        "\n",
        "    # prende un tensore di patch e lo converte in un formato di immagine\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size)\n",
        "        imgs: (N, 1, num_voxels)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size\n",
        "        h = x.shape[1]\n",
        "\n",
        "        imgs = x.reshape(shape=(x.shape[0], -1, x.shape[2] // p))\n",
        "        return imgs.transpose(1, 2)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        if self.focus_range is not None:\n",
        "            len_mask = L - len_keep\n",
        "            weights = [1 - self.focus_rate] * L\n",
        "            weights[\n",
        "                self.focus_range[0]\n",
        "                // self.patch_size: self.focus_range[1]\n",
        "                // self.patch_size\n",
        "            ] = [self.focus_rate] * (\n",
        "                self.focus_range[1] // self.patch_size\n",
        "                - self.focus_range[0] // self.patch_size\n",
        "            )\n",
        "            weights = torch.tensor(weights).repeat(N, 1).to(x.device)\n",
        "            ids_mask = torch.multinomial(weights, len_mask, replacement=False)\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        if self.focus_range is not None:\n",
        "            for i in range(N):\n",
        "                noise[i, ids_mask[i, :]] = 1.1  # set mask portion to 1.1\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(\n",
        "            noise, dim=1\n",
        "        )  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(\n",
        "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        #x torch.Size([1, 1024, 128])\n",
        "\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)  # patch di input nel modello torch.Size([1, 128, 1024])\n",
        "\n",
        "        # add pos embed with out the first column that is for the cls token\n",
        "        x = (\n",
        "            x + self.pos_embed[:, 1:, :] \n",
        "        )  # Aggiunge gli embedding posizionali senza il token di classe\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(\n",
        "            x, mask_ratio\n",
        "        )  # Applica una mascheratura casuale all'input e restituisce l'output mascherato\n",
        "        #x torch.Size([1, 115, 1024])\n",
        "\n",
        "        # Crea un token di classe con l'embedding posizionale torch.Size([1, 1, 1024])\n",
        "        cls_token = (\n",
        "            self.cls_token + self.pos_embed[:, :1, :]\n",
        "        ) \n",
        "\n",
        "        # Espande il token di classe per adattarlo alle dimensioni dell'input torch.Size([1, 116, 1024])\n",
        "        cls_tokens = cls_token.expand(\n",
        "            x.shape[0], -1, -1\n",
        "        )  \n",
        "        # Aggiunge il token di classe all'inizio delle patch\n",
        "        x = torch.cat(\n",
        "            (cls_tokens, x), dim=1\n",
        "        )  \n",
        "        # x torch.Size([1, 116, 1024])\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(\n",
        "                x\n",
        "            )  # Applica una serie di blocchi Transformer (self.blocks) all'input\n",
        "        # Normalizza l'output utilizzando la normalizzazione di layer\n",
        "        # x torch.Size([1, 116, 1024])\n",
        "        x = self.norm(x)\n",
        "        \n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    \n",
        "\n",
        "    def forward_decoder(self, x, ids_restore=None):\n",
        "        #x torch.Size([1, 116, 1024])\n",
        "\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x) #torch.Size([1, 116, 512])\n",
        "        # print('decoder embed')\n",
        "        # print(x.shape)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(\n",
        "            x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1\n",
        "        )  # Concatena i token di decodifica e i token di maschera,\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "\n",
        "        # x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(\n",
        "            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n",
        "        )  # unshuffle #Riordina la sequenza basandosi sugli indici per ripristinare l'ordine originale\n",
        "\n",
        "        x = torch.cat(\n",
        "            [x[:, :1, :], x_], dim=1\n",
        "        )  # append cls token # Aggiunge nuovamente il token di classe alla sequenza\n",
        "        # x = x_\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed  # Aggiunge embedding posizionali\n",
        "        # x = x + self.decoder_pos_embed[:, 1:, :]\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)  # Applica una serie di blocchi\n",
        "        x = self.decoder_norm(\n",
        "            x\n",
        "        )  # Normalizza l'output utilizzando la normalizzazione di layer\n",
        "        # print(x.shape)\n",
        "        # predictor projection\n",
        "        # Implementando una trasformazione lineare (nn.Linear) Proietta l'output \n",
        "        x = self.decoder_pred(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]  # Rimuove il token di classe dall'output\n",
        "\n",
        "        return x  # Restituisce l'output decodificato\n",
        "\n",
        "  \n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 1, num_voxels]\n",
        "        imgs: [N, chan, T]\n",
        "        pred: [N, L, p]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        imgs = imgs.transpose(1, 2)\n",
        "        target = self.patchify(imgs)\n",
        "        # target = imgs.transpose(1,2)\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], Mean Squared Error per patch\n",
        "        # loss = loss.mean()\n",
        "        loss = (\n",
        "            (loss * mask).sum() /\n",
        "            mask.sum() if mask.sum() != 0 else (loss * mask).sum()\n",
        "        )  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs,  mask_ratio=0.75):\n",
        "        #imgs torch.Size([1, 128, 512])\n",
        "\n",
        "        # latent = self.forward_encoder(imgs, mask_ratio)\n",
        "        latent, mask, ids_restore = self.forward_encoder(\n",
        "            imgs, mask_ratio\n",
        "        )  # Esegue la codifica delle immagini di input masked\n",
        "\n",
        "        # latent torch.Size([1, 116, 1024])\n",
        "        \n",
        "        pred = self.forward_decoder(\n",
        "            latent, ids_restore\n",
        "        )  # [N, L, p] # Esegue la decodifica\n",
        "        # pred torch.Size([1, 128, 512])\n",
        "       \n",
        "        loss = self.forward_loss(\n",
        "            imgs, pred, mask\n",
        "        )  # Calcola la perdita basata sulle immagini originali e le predizioni decodificate\n",
        "        # print(self.unpatchify(pred.transpose(1,2)).shape)\n",
        "\n",
        "        return loss, pred, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to pretrain the model of the MAE we:\n",
        "1. load the dataset containing large amount of EEG data and the dataloader to iterate on them during training. \n",
        "2. initialize the model using the parameters of the EEG data (QUALI PARAMETRI?). \n",
        "3. Initialize an optimizer AdamW to optimize model's parameters using also a weight decay during optimization.\n",
        "4. Start the training in which during an epoch a batch of data is retrieved, and passed to the model obtaining the loss and the predicted output. So we execute the gradient backpropagation calculated on the loss and we update the parameters of the model. (?The mean correlation between the prediction and the input?)\n",
        "5. Once the training is finished we save the checkpoint of the model in order to use it in the next step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(config):\n",
        "    device = torch.device(\"cpu\")\n",
        "    torch.manual_seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "\n",
        "    # Large amount of EEG data are loaded\n",
        "    loadEEG_5_95()\n",
        "    \n",
        "    # Creation of dataset and dataloader\n",
        "    dataset_pretrain = eeg_pretrain_dataset(\n",
        "        path=\"datasets/eegdataset/eeg\",\n",
        "    )\n",
        "    dataloader_eeg = DataLoader(\n",
        "        dataset_pretrain,\n",
        "        batch_size=config.batch_size,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Initialization of Masekd AutoEncoder model\n",
        "    config.time_len = dataset_pretrain.data_len\n",
        "    model = MaskedAutoEncoderEEG(\n",
        "        time_len=dataset_pretrain.data_len,\n",
        "        patch_size=config.patch_size,\n",
        "        embed_dim=config.embed_dim,\n",
        "        decoder_embed_dim=config.decoder_embed_dim,\n",
        "        depth=config.depth,\n",
        "        num_heads=config.num_heads,\n",
        "        decoder_num_heads=config.decoder_num_heads,\n",
        "        mlp_ratio=config.mlp_ratio,\n",
        "        focus_range=config.focus_range,\n",
        "        focus_rate=config.focus_rate,\n",
        "        img_recon_weight=config.img_recon_weight,\n",
        "    )\n",
        "    model_without_ddp = model\n",
        "\n",
        "    # Manage weight decay during optimization.\n",
        "    param_groups = optim_factory.add_weight_decay(\n",
        "        model, config.weight_decay\n",
        "    )  \n",
        "    \n",
        "    # AdamW optimizer instantiated\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        param_groups, lr=config.lr, betas=(0.9, 0.95)\n",
        "    )  \n",
        "\n",
        "    cor_list = []  # list to track correlation during training\n",
        "    start_time = time.time()\n",
        "    print(\"Start Training the EEG MAE ... ...\")\n",
        "\n",
        "    for ep in range(config.num_epoch):\n",
        "        print(f\"Currently on Epoch {ep} ...\")\n",
        "        # Training step\n",
        "        cor = train_one_epoch(\n",
        "            model,\n",
        "            dataloader_eeg,\n",
        "            optimizer,\n",
        "            device,\n",
        "            ep,\n",
        "            config,\n",
        "            model_without_ddp\n",
        "        )\n",
        "        cor_list.append(cor)\n",
        "        if (\n",
        "            ep % 20 == 0 or ep + 1 == config.num_epoch\n",
        "        ) and config.local_rank == 0:  # and ep != 0\n",
        "            # save models\n",
        "            save_model(\n",
        "                config,\n",
        "                ep,\n",
        "                model_without_ddp,\n",
        "                optimizer,\n",
        "                os.path.join(config.output_path, \"checkpoints\"),\n",
        "            )\n",
        "            # plot figures\n",
        "            plot_recon_figures(\n",
        "                model,\n",
        "                device,\n",
        "                dataset_pretrain,\n",
        "                config.output_path,\n",
        "                5,\n",
        "                config,\n",
        "                None,\n",
        "                model_without_ddp,\n",
        "            )\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print(\"Training time {}\".format(total_time_str))\n",
        "    return\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    data_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    epoch,\n",
        "    config=None,\n",
        "    model_without_ddp=None\n",
        "):\n",
        "    # Set the model to training mode\n",
        "    model.train(True)\n",
        "    optimizer.zero_grad()  # Resets the optimizer gradients\n",
        "    total_loss = []  # Loss accumulator\n",
        "    total_cor = []  # Correlations accumulator\n",
        "\n",
        "    # Iterating through the dataloader\n",
        "    data_dcit = next(iter(data_loader), None)\n",
        "    # Adjusting the learning rate based on the number of iterations\n",
        "    ut.adjust_learning_rate(optimizer, epoch, config)\n",
        "\n",
        "    samples = data_dcit[\"eeg\"]  # Samples of EEG from dataloader torch.Size([1, 128, 512])\n",
        "    samples = samples  # Move samples to device\n",
        "\n",
        "    optimizer.zero_grad()  # Resets the optimizer gradients\n",
        "\n",
        "    # Enabling automatic autocasting mode to perform mixed precision operations during forward and backward pass of the model\n",
        "    # Execute forward passof the model \n",
        "    with torch.autocast(device_type=\"cpu\"):\n",
        "       loss, pred, _ = model(samples, mask_ratio=config.mask_ratio)  \n",
        "    \n",
        "    loss_value = loss.item()\n",
        "    \n",
        "    if not math.isfinite(loss_value):\n",
        "        print(f\"Loss is not finite: {loss_value}, stopping training at epoch {epoch}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # esegue __call__ di NativeScalerWithGradNormCount e modifica i gradienti\n",
        "    # loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad) \n",
        "    # cal the cor\n",
        "    print(\"Loss: \"+str(loss_value))\n",
        "    # Backward operation\n",
        "    loss.backward(create_graph=False)   \n",
        "    optimizer.step()\n",
        "    \n",
        "    pred = pred.to(\"cpu\").detach()\n",
        "    samples = samples.to(\"cpu\").detach()\n",
        "  \n",
        "    # Transform tensor in image\n",
        "    pred = model_without_ddp.unpatchify(pred)\n",
        "  \n",
        "    # The average correlation between the predictions (pred) and the input samples (samples) is calculated\n",
        "    cor = torch.mean(\n",
        "        torch.tensor(\n",
        "            [\n",
        "                torch.corrcoef(\n",
        "                    torch.cat(\n",
        "                        [p[0].unsqueeze(0), s[0].unsqueeze(0)], axis=0)\n",
        "                )[0, 1]\n",
        "                for p, s in zip(pred, samples)\n",
        "            ]\n",
        "        )\n",
        "    ).item()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss.append(loss_value)\n",
        "    total_cor.append(cor)\n",
        "    if device == torch.device(\"cuda:0\"):\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(\n",
        "            \"train_loss_step:\",\n",
        "            np.mean(total_loss),\n",
        "            \"lr:\",\n",
        "            lr,\n",
        "            \"cor\",\n",
        "            np.mean(total_cor),\n",
        "        )\n",
        "\n",
        "    return np.mean(total_cor)\n",
        "\n",
        "def save_model(config, epoch, model, optimizer, checkpoint_paths):\n",
        "    os.makedirs(checkpoint_paths, exist_ok=True)\n",
        "    to_save = {\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'config': config,\n",
        "    }\n",
        "    torch.save(to_save, os.path.join(checkpoint_paths, 'pretrain.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tuning with limited EEG-image pairs with pre-trained Stable Diffusion and allignment of EEG, text, and image spaces using CLIP encoders\n",
        "We implement the second and the third sections described in the paper in this step. We implement the procuderes of a StableDiffusion model (generative model) laveraging our MAE pretrained model, obtained in the previous step, and the CLIP's image-text encodings in order to fine-tune a model which can generate high definition images. \n",
        "First of all we load the dataset of EEG-images paired data to create out training set. (?Trasformazione delle immagini?). Then we initialize the following things: \n",
        "* EEG Encoder using the weight from our MAE pretrained model.\n",
        "* UNet: is a convolutional neural network whose architecture work well with fewer training images and yield more precise segmentation.\n",
        "* VAE: is an autoencoder model that incorporates the Kullback-Leibler (KL) divergence as part of its training objective. The Kullback-Leibler (KL) divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In the context of autoencoders, particularly in variational autoencoders (VAEs), the KL divergence is used to regularize the latent space by encouraging the learned latent distribution to be close to a predefined prior distribution, typically a multivariate Gaussian distribution.\n",
        "* PNDMScheduler: defines how to iteratively add noise to a latent during diffusion process. \n",
        "* CLIP: is a model based on the Transformer architecture which multi-head attention mechanism allows it to capture complex relationships between words and tokens in the input.\n",
        "* two Projection Layer: (?DESCRIZIONE E A COSA SERVONO?)\n",
        "\n",
        "We use an accelerator to prepare all the objects and models to training and we declare an optimizer AdamW for the training of the model UNet, EED Encoder and one of the Projector. \n",
        "Then we start the training in which, in each epoch, we process a batch of the training data which contains the EEG and the relative image. EEG data is processed using the encoder model to extract relevant features. The extracted features are then used to project the hidden space through the Projector model. The VAE model is used to encode the image into latent data. This latent data is then modified and distorted by adding noise, with the same shape as the latents extracted from the image, and temporal changes through the PNDMScheduler model. The distorted latent data and the hiddent states derived from Projector model are then passed through the UNet model, which is used to generate a new image based on the inputs. The loss function of this part of the model is a Mean Square Error function between the generation of the UNet model and the noise.\n",
        "The training data batch's image is also cropped and passed to CLIP model in order to generate embeddings. Then we calculate the loss function for CLIP encoding using a cosine similarity between encoded image and the projection of the hidden space. \n",
        "Then we consider as the total loss of the entire model the sum of the UNet loss and the CLIP loss. Backpropagation is performed to calculate the gradient of the loss function with respect to the model parameters. The gradient is then used to update the model weights through the optimizer.\n",
        "Once the process is finished we save the checkpoint of the model, saving the states of the different models used: UNet, EEG Encoder, VAE, CLIP, Projector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, device):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.device = device\n",
        "        self.l1 =nn.Linear(768,384)\n",
        "        self.projection = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x).to(self.device)\n",
        "        # Reshape the input tensor to have a single batch dimension\n",
        "        x = self.projection(x.flatten()).to(self.device)\n",
        "        # Apply the linear projection\n",
        "\n",
        "        return torch.reshape(x,(1,768))\n",
        "    \n",
        "class ProjectionLayerEmbedding(nn.Module):\n",
        "    def __init__(self, input_size, output_size, device):\n",
        "        super(ProjectionLayerEmbedding, self).__init__()\n",
        "        self.device = device\n",
        "        self.projection = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape the input tensor to have a single batch dimension\n",
        "        x = self.projection(x.flatten()).to(self.device)\n",
        "        # Apply the linear projection\n",
        "\n",
        "        return torch.reshape(x,(77,768))\n",
        "     \n",
        "def main(config):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Pretrianed model loading\n",
        "    pretrain_model = torch.load(config.pretrain_mbm_path, map_location=device)\n",
        "    metafile_config = pretrain_model['config']\n",
        "\n",
        "    # Training image transformations\n",
        "    img_transform_train = transforms.Compose([\n",
        "        ut.normalize,\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomCrop(size=(512, 512)),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "        # transforms.Resize((512, 512)),\n",
        "        ut.channel_last,\n",
        "    ])\n",
        "\n",
        "    # Testing image transformations\n",
        "    img_transform_test = transforms.Compose([\n",
        "        ut.normalize,\n",
        "        transforms.Resize((512, 512)),\n",
        "        ut.channel_last\n",
        "    ])\n",
        "\n",
        "    # Crop image transformation for CLIP model\n",
        "    crop_transform = transforms.CenterCrop((224, 224))\n",
        "\n",
        "    # Definition of train and test datasets\n",
        "    eeg_latents_dataset_train, _ = create_EEG_dataset(\n",
        "        eeg_signals_path=config.eeg_signals_path,\n",
        "        splits_path=config.splits_path,\n",
        "        image_transform=[img_transform_train, img_transform_test],\n",
        "        subject=config.subject,\n",
        "    )\n",
        "    data_len_eeg = eeg_latents_dataset_train.data_len\n",
        "\n",
        "    # Definition of the models useful for fine-tuning\n",
        "    encoder = eegEncoder(time_len=data_len_eeg, patch_size=metafile_config.patch_size, embed_dim=metafile_config.embed_dim,\n",
        "                         depth=metafile_config.depth, num_heads=metafile_config.num_heads, mlp_ratio=metafile_config.mlp_ratio)\n",
        "\n",
        "    encoder.load_checkpoint(pretrain_model['model'])\n",
        "\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "       \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "       \"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "\n",
        "    scheduler = PNDMScheduler.from_pretrained(\n",
        "       \"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "    clip_model, _ = CLIP.load(name=\"ViT-L/14\", device=device)\n",
        "    projector1 = ProjectionLayerEmbedding(128*metafile_config.embed_dim, 59136, device)\n",
        "    projection_layer = ProjectionLayer(29568, 768, device)\n",
        "\n",
        "    vae.required_grad = False\n",
        "    clip_model.required_grad = False\n",
        "    projection_layer.required_grad = False\n",
        "\n",
        "    # Accelerator\n",
        "    accelerator = Accelerator()\n",
        "    unet, encoder, vae, scheduler, projector1, clip_model, crop_transform = accelerator.prepare(\n",
        "        unet, encoder, vae, scheduler, projector1, clip_model, crop_transform)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(list(unet.parameters()) +\n",
        "                                  list(encoder.parameters()) +\n",
        "                                  list(projector1.parameters()), lr=config.lr)\n",
        "    try:\n",
        "        # Fine-tuning loop\n",
        "        for epoch in range(config.num_epoch):\n",
        "            current_dateTime = datetime.datetime.now()\n",
        "            print(\"Starting Date: \" + str(current_dateTime))\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "\n",
        "            for step, batch in enumerate(eeg_latents_dataset_train):\n",
        "                eeg = batch[\"eeg\"]\n",
        "                image = batch[\"image\"]\n",
        "\n",
        "                with accelerator.accumulate(unet, encoder, projector1):\n",
        "                    # EEG embedding are retrieved using encoder\n",
        "                    embeddings = encoder(eeg)\n",
        "                    # Embeddings are projected \n",
        "                    hidden_states = projector1(embeddings)\n",
        "                    # Image is prepared for encoding\n",
        "                    image_for_encode = change_shape_for_encode(\n",
        "                        image)\n",
        "                    \n",
        "                    # The VAE model is used to encode the image into latent data\n",
        "                    latents = vae.encode(image_for_encode).latent_dist.sample()\n",
        "                    latents = latents * vae.config.scaling_factor\n",
        "\n",
        "                    # A random noise is added to latents\n",
        "                    noise = torch.randn_like(latents)\n",
        "                    timesteps = torch.randint(0, 1000, (1,),\n",
        "                                            device=device).long()\n",
        "                    noisy_latents = scheduler.add_noise(\n",
        "                        latents, noise, timesteps)\n",
        "\n",
        "                    # An image is generated using UNet model\n",
        "                    model_pred = unet(noisy_latents, timesteps, hidden_states.unsqueeze(0), return_dict=False)[\n",
        "                        0]\n",
        "                    # Loss function for UNet = Mean Square Error\n",
        "                    loss_unet = F.mse_loss(\n",
        "                        model_pred, noise, reduction=\"mean\")\n",
        "\n",
        "                    # Image preparation for CLIP model\n",
        "                    image_cropped = crop_transform(image_for_encode)\n",
        "                    # Encoding of the image using CLIP model\n",
        "                    image_encoded = clip_model.encode_image(\n",
        "                        image_cropped)\n",
        "\n",
        "                    # Clip loss function = 1 - cosine similarity between encoded image from CLIP and hidden states\n",
        "                    projection = projection_layer(hidden_states)\n",
        "                    loss_clip = 1 - F.cosine_similarity(image_encoded, projection)\n",
        "\n",
        "                    # Total loss of the model is the sum of UNet's loss and CLIP's loss\n",
        "                    total_loss = loss_unet + loss_clip\n",
        "                    if not math.isfinite(total_loss.item()):\n",
        "                        exit(1)\n",
        "\n",
        "                    # Backpropagation\n",
        "                    accelerator.backward(total_loss)\n",
        "                    current_dateTime = datetime.datetime.now()\n",
        "                    print(str(total_loss.item()) + \" Step: \" + str(step) + \" \" + str(current_dateTime))\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            current_dateTime = datetime.datetime.now()\n",
        "            print(\"End Date: \" + str(current_dateTime))\n",
        "        save_model(unet, encoder, vae, clip_model, projector1, config, config.output_path, epoch)\n",
        "      \n",
        "def change_shape_for_encode(image):\n",
        "    return rearrange(image, \"h w c-> c h w\").unsqueeze(0).to(dtype=torch.float32)\n",
        "\n",
        "def save_model(unet, encoder, vae, clip_model, projector1, config, output_path, epoch=None):\n",
        "    torch.save(\n",
        "        {\n",
        "            'unet_state_dict': unet.state_dict(),\n",
        "            'egg_encoder_state_dict': encoder.state_dict(),\n",
        "            'vae_state_dict': vae.state_dict(),\n",
        "            'clip_model_state_dict': clip_model.state_dict(),\n",
        "            'projector1': projector1.state_dict(),\n",
        "            'config': config,\n",
        "            'state': torch.random.get_rng_state()\n",
        "        },\n",
        "        os.path.join(output_path, f'generation.pth')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image generation\n",
        "The last step is to generate images starting from a test set of EEG signals laveraging the fine tuned Stable Diffusion model obtained in the previous step.\n",
        "The procedure used to generate images is similar from the previous section.\n",
        "We define a ProjectionLayerEmbedding class that represents a linear projection layer used for embedding data. The forward method takes a tensor x, flattens it, projects it linearly, and then scales it to the form (77, 768).\n",
        "We start by defining image transformations for training and testing. These transformations include normalizing, resizing, and cropping images. The EEG dataset for test is created. An accelerator is configured for training the model. The pre-trained model for EEG embedding of the first section is loaded and its features are extracted to define the encoder for EEG data. The generative model, taken from the second section, is loaded and the different states included are estracted.  \n",
        "Additional models and components needed for the image generation process, such as an encoder, a decoder, a UNet model, a CLIP model, and a projector, are instantiated and loaded.\n",
        "Then a loop iterates through the test dataset. For each batch, the EEG signal and the corresponding image are extracted. EEG embeddings are then calculated using the encoder, and these embeddings are projected into the appropriate space using Projector. The image is formatted to be ready for encoding. The formatted image is encoded using the VAE (vae) model. A latent is extracted by sampling from the latent distribution. An initial (input) noise is generated using a normal distribution, with the same shape as the latents extracted from the image.\n",
        "We set the number of timesteps, the initial noise is modified using the UNet model and the scheduler to obtain an initial image. This image is then used as input for the next step. This process is iterated through all time steps.\n",
        "The generated image is decoded using the VAE decoder. The output is an image sampled from the distribution of images generated by the model.\n",
        "\n",
        "The image retalted to the EEG data used during generation and generated images are saved in order to analyze the results and take considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProjectionLayerEmbedding(nn.Module):\n",
        "    def __init__(self, input_size, output_size, device):\n",
        "        self.device = device\n",
        "        super(ProjectionLayerEmbedding, self).__init__()\n",
        "        self.projection = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape the input tensor to have a single batch dimension\n",
        "        x = self.projection(x.flatten()).to(self.device)\n",
        "        # Apply the linear projection\n",
        "        return torch.reshape(x, (77, 768))\n",
        "\n",
        "def main(config):\n",
        "    logging_dir = os.path.join(config.output_path, \"accLog/\")\n",
        "    # Pretrianed model loading\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    pretrain_model = torch.load('pretrains\\eeg_pretrain\\checkpoint.pth', map_location=device)\n",
        "    metafile_config = pretrain_model['config']\n",
        "\n",
        "    # Training image transformations\n",
        "    img_transform_train = transforms.Compose(\n",
        "        [\n",
        "            ut.normalize,\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.RandomCrop(size=(512, 512)),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "            ut.channel_last,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Testing image transformations\n",
        "    img_transform_test = transforms.Compose(\n",
        "        [ut.normalize, transforms.Resize((512, 512)), ut.channel_last]\n",
        "    )\n",
        "\n",
        "    # Definition of train and test datasets\n",
        "    _, eeg_latents_dataset_test = create_EEG_dataset(\n",
        "        eeg_signals_path=config.eeg_signals_path,\n",
        "        splits_path=config.splits_path,\n",
        "        image_transform=[img_transform_train, img_transform_test],\n",
        "        subject=config.subject,\n",
        "    )\n",
        "    data_len_eeg = eeg_latents_dataset_test.data_len\n",
        "    # Accelerator\n",
        "    accelerator_project_config = ProjectConfiguration(\n",
        "        project_dir=config.output_path, logging_dir=logging_dir)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=1,\n",
        "        mixed_precision=None,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_config=accelerator_project_config,\n",
        "    )\n",
        "    # Finetuned model loading\n",
        "    folder_path = \"exps/results/generation/\"\n",
        "    model_name = \"generation.pth\"\n",
        "    model_path = Path(folder_path) / model_name\n",
        "    generative_model = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Definition of the models useful for generation\n",
        "    encoder = eegEncoder(time_len=data_len_eeg, patch_size=metafile_config.patch_size, embed_dim=metafile_config.embed_dim,\n",
        "                         depth=metafile_config.depth, num_heads=metafile_config.num_heads, mlp_ratio=metafile_config.mlp_ratio)\n",
        "\n",
        "    scheduler = PNDMScheduler.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "    projector1 = ProjectionLayerEmbedding(128*metafile_config.embed_dim, 59136, device)\n",
        "\n",
        "    # Loading of the models' states\n",
        "    unet.load_state_dict(generative_model['unet_state_dict'])\n",
        "    encoder.load_state_dict(generative_model['egg_encoder_state_dict'])\n",
        "    vae.load_state_dict(generative_model['vae_state_dict'])\n",
        "    projector1.load_state_dict(generative_model['projector1'])\n",
        "\n",
        "    for step, batch in enumerate(eeg_latents_dataset_test):\n",
        "        eeg = batch[\"eeg\"]\n",
        "        image = batch[\"image\"]\n",
        "        # EEG related image is saved\n",
        "        image_real = Image.fromarray(\n",
        "            ((image.cpu() / 2 + 0.5).clamp(0, 1) * 255).numpy().astype('uint8'))\n",
        "        image_real_path = os.path.join(config.output_path,\n",
        "                                    f'testset/real/{step}.jpg')\n",
        "        image_real.save(image_real_path)\n",
        "\n",
        "        # EEG embedding are retrieved using encoder\n",
        "        embeddings = encoder(eeg)\n",
        "        # Embeddings are projected \n",
        "        hidden_states = projector1(embeddings)\n",
        "        # Image is prepared for encoding\n",
        "        image_for_encode = change_shape_for_encode(image)\n",
        "        del embeddings, eeg, image\n",
        "        \n",
        "        # The VAE model is used to encode the image into latent data\n",
        "        latents = vae.encode(image_for_encode).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "        \n",
        "        # A random noise is initialized\n",
        "        input = torch.randn_like(latents).to(accelerator.device)\n",
        "        del latents, image_for_encode\n",
        "        \n",
        "        # Definition of timesteps\n",
        "        timesteps = 199\n",
        "        scheduler.set_timesteps(timesteps)\n",
        "        for t in tqdm(scheduler.timesteps):\n",
        "            with torch.no_grad():\n",
        "                # Generating image using UNet and Scheduler  \n",
        "                noisy_residual = unet(input, t, hidden_states.unsqueeze(0), return_dict=False)[0]\n",
        "                prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n",
        "                # Generated image is used as input of the next iteration\n",
        "                input = prev_noisy_sample\n",
        "        del timesteps, hidden_states, noisy_residual\n",
        "        input = 1 / 0.18215 * input\n",
        "        with torch.no_grad():\n",
        "            # Generated image is decoded thanks to VAE decoder\n",
        "            image_gen = vae.decode(input).sample\n",
        "\n",
        "        # Generated image is saved\n",
        "        image_gen = (image_gen / 2 + 0.5).clamp(0, 1)\n",
        "        image_gen = image_gen.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
        "        image_gen = Image.fromarray((image_gen * 255).round().astype(\"uint8\"))\n",
        "\n",
        "        image_gen_path = os.path.join(config.output_path,\n",
        "                                    f'testset/test/{step}.jpg')\n",
        "\n",
        "        image_gen.save(image_gen_path)\n",
        "\n",
        "def change_shape_for_encode(image):\n",
        "    return rearrange(image, \"h w c-> c h w\").unsqueeze(0).to(dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the work\n",
        "We evaluate the effectiveness of our implementation by employing a accuracy classification task. We use a pre-trained ImageNet1K classifier, [MaxViT](https://arxiv.org/abs/2204.01697), to determine the semantic correctness of the generated images. MaxViT is a family of hybrid (CNN + ViT) image classification models, that achieves better performances across the board for both parameter and FLOPs efficiency than both SoTA ConvNets and Transformers. MaxVit family models have been trained on ImageNet1K, a subset of the larger ImageNet set, which images covers 1000 classes. We remember that our EEG-image pairs are obtained from 50 classes present in these set of images. There are different MaxVit models, they are trained on different image resolution (224x224, 384x384, 512x512) and with different size of parameters. These models are MaxViT-T (Tiny) with 31M parameters, MaxViT-S (Small) with 69M, MaxViT-B (Base) with 119M parameters and MaxViT-L (Large) with 212M parameters.\n",
        "The classifier will find the five most probable image classes that it will attribute to the input image, in order of decreasing probability.\n",
        "Both the ground-truth and generated images will be inputted into the classifier. We will verify two evaluation:\n",
        "1. Top-1 classification: a generated image will be deemed correct as the most probable semantic classification results of the generated image and the ground-truth are consistent.\n",
        "2. Top-5 classification: a generated image will be deemed correct as the most probable semantic classification result of the ground-truth and one of the five semantic classification result of the generated image are consistent.\n",
        "We exploit the different Maxvit models with its various sizes and resolutions of the training images by combining them with the various image infer sizes, the dimension of the image used during inference that must be a multiple of the resolution used in the model.\n",
        "We can resume our analysis with these tables:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NN_Project_CIFAR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25545e06035542b9a03bdea24d4d3719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3716ae13d34045dd9efb5c12766ae0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a787a36248747328a5b56c8bc40a7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6016ee04501c4752a806e1be2bd47552",
              "IPY_MODEL_f9c1862ed87241afa4627270bfc7e7ac",
              "IPY_MODEL_58dd89ddf9f64e989a61f11b00075dd9"
            ],
            "layout": "IPY_MODEL_3716ae13d34045dd9efb5c12766ae0ed"
          }
        },
        "58dd89ddf9f64e989a61f11b00075dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e36fbc58dca64d989a0eee265aa34430",
            "placeholder": "​",
            "style": "IPY_MODEL_62a706c20f8349d9b375e619120fabdb",
            "value": " 170499072/? [00:06&lt;00:00, 31948147.81it/s]"
          }
        },
        "6016ee04501c4752a806e1be2bd47552": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c460f2eea54073a7b41fd83648f1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_9dfcec1ccdc74f2f8d1336a02c68937c",
            "value": ""
          }
        },
        "62a706c20f8349d9b375e619120fabdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dfcec1ccdc74f2f8d1336a02c68937c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3153164a6d4f79abec2625968910f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e36fbc58dca64d989a0eee265aa34430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c460f2eea54073a7b41fd83648f1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c1862ed87241afa4627270bfc7e7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3153164a6d4f79abec2625968910f2",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25545e06035542b9a03bdea24d4d3719",
            "value": 170498071
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
